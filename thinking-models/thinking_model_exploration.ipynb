{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeDAZVEq9fLg",
        "outputId": "606a3af4-e3b2-4dda-a1eb-b18f9a6c3be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running as a Colab notebook\n"
          ]
        }
      ],
      "source": [
        "# Configs\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Not running as a Colab notebook\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "# !pip install sparsify\n",
        "# !pip install sparsezoo.analyze # sparsify dependency\n",
        "\n",
        "# !pip install sparsify-nightly"
      ],
      "metadata": {
        "id": "wwcxYKm7A6EC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "iKATrmnc9fLn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import ollama\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# from tl_transformers.integrations.sparse_autoencoder import SparseAutoencoder\n",
        "\n",
        "# from huggingface_hub import hf_hub_download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDAzfSxg9fLo"
      },
      "outputs": [],
      "source": [
        "# Ollama usage\n",
        "\n",
        "# Basic usage\n",
        "# response = ollama.chat(model='deepseek-r1:8b', messages=[\n",
        "#     {\n",
        "#         'role': 'user',\n",
        "#         'content': 'Solve this step by step: What is 25 x 16?'\n",
        "#     }\n",
        "# ])\n",
        "# print(response['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TtFbOH0q9fLq"
      },
      "outputs": [],
      "source": [
        "def load_llama_3_8B_model():\n",
        "    model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def load_r1_8B_model(): pass\n",
        "\n",
        "def load_llama_scope_sae():\n",
        "    model_name = \"fnlp/Llama-Scope\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    return tokenizer, model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# llama_scope_sae_tokenizer, llama_scope_sae_model = load_llama_scope_sae()"
      ],
      "metadata": {
        "id": "CDVPYr0d_heF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "sZBoRNOt9fLq",
        "outputId": "db1a9c2c-b5fd-494e-f075-c502c6a80140"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bdd7ff6e9358>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgsm8k_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gsm8k\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"main\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgsm8k_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "# gsm8k_dataset = load_dataset(\"gsm8k\", \"main\")\n",
        "# print(gsm8k_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veaRLeVu9fLr",
        "outputId": "7621ada8-ea84-4425-dac6-80856fdfa91e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question\n",
            "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
            "Answer\n",
            "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
            "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
            "#### 72\n"
          ]
        }
      ],
      "source": [
        "# print(\"Question\")\n",
        "# print(gsm8k_dataset[\"train\"][0][\"question\"])\n",
        "# print(\"Answer\")\n",
        "# print(gsm8k_dataset[\"train\"][0][\"answer\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llms",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}