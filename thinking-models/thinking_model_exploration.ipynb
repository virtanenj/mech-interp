{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Thinking model mech interp exploration"
      ],
      "metadata": {
        "id": "KSdFBFqtqCht"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeDAZVEq9fLg"
      },
      "outputs": [],
      "source": [
        "# Configs\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "    !pip install -q transformers\n",
        "    !pip install -q datasets\n",
        "    !pip install -q huggingface_hub\n",
        "    !pip install -q sae-lens\n",
        "    !pip install -q transformer-lens\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Not running as a Colab notebook. Make sure to have required packages installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iKATrmnc9fLn"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import pipeline # Pipeline needed to create text-generations\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import hf_hub_download, notebook_login\n",
        "\n",
        "from sae_lens import SAE\n",
        "\n",
        "import joblib # To offload data or models to disk while not using them\n",
        "\n",
        "import gc # Garbage collection. E.g. use `gc.collect()`\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "notebook_login() # Login (Llama 3 8B requires licence agreement access)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setups"
      ],
      "metadata": {
        "id": "fBYRe6W-p217"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TtFbOH0q9fLq"
      },
      "outputs": [],
      "source": [
        "# Nb requires Hugging Face login\n",
        "def load_llama_3_8B_model():\n",
        "    model_name = \"meta-llama/Llama-3.1-8B\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "def load_r1_8B_model():\n",
        "    model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "def load_gsm8k_dataset():\n",
        "    dataset_name = \"openai/gsm8k\"\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    return dataset\n",
        "\n",
        "def load_mmlu_dataset(config_name=\"elementary_mathematics\", dataset_size=None):\n",
        "    dataset_name = \"cais/mmlu\"\n",
        "    dataset = load_dataset(dataset_name, config_name)\n",
        "    return dataset\n",
        "\n",
        "# Download Llama Scope SAEs for model Llama-3.1-8B\n",
        "def load_llama_scope_sae(layer_idx, layer_type, sae_dim, device=\"cpu\"):\n",
        "    if not (0 <= layer_idx <= 31 and isinstance(layer_idx, int)):\n",
        "        raise ValueError(\"Invalid layer index. Must be an integer between 0 and 31.\")\n",
        "    if layer_type not in (\"a\", \"m\", \"r\"):\n",
        "        raise ValueError(\"Invalid SAE layer type. layer_type must be 'a', 'm', or 'r'.\")\n",
        "    if sae_dim not in (8, 32):\n",
        "        raise ValueError(\"Invalid SAE dimension. sae_dim must be 8 or 32.\")\n",
        "    release = \"llama_scope_lx{}_{}x\".format(layer_type, sae_dim)\n",
        "    sae_id = \"l{}{}_{}x\".format(layer_idx, layer_type, sae_dim)\n",
        "    sae = SAE.from_pretrained(release, sae_id, device=device)[0]\n",
        "    return sae\n",
        "\n",
        "# Downloads Llama Scopre SAEs for model DeepSeek-R1-Distill-Llama-8B\n",
        "def load_sae_llama_scope_r1_distill(layer_idx, device=\"cpu\"):\n",
        "    if not (0 <= layer_idx <= 31 and isinstance(layer_idx, int)):\n",
        "        raise ValueError(\"Invalid layer index. Must be an integer between 0 and 31.\")\n",
        "    release = \"llama_scope_r1_distill\"\n",
        "    sae_id = \"l{}r_800m_slimpajama\".format(layer_idx)\n",
        "    sae = SAE.from_pretrained(release, sae_id, device=\"cpu\")[0]\n",
        "    return sae"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sae = load_llama_scope_sae(0, \"m\", 8)"
      ],
      "metadata": {
        "id": "ipHEtNAtVOvd"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mmlu_dataset = load_mmlu_dataset()"
      ],
      "metadata": {
        "id": "FMt5nSxWLXY3"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama_tokenizer, llama_model = load_llama_3_8B_model()"
      ],
      "metadata": {
        "id": "lnx6n1t8lgOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llama_model)"
      ],
      "metadata": {
        "id": "S_M1L8OTrMtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llama_model.base_model.layers[15].mlp)"
      ],
      "metadata": {
        "id": "sOCt0BYcrPrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text generation\n",
        "input_text = \"Consider the equation 3x - 1 = 2. What is x? Answer: x = \"\n",
        "input_ids = llama_tokenizer(input_text, return_tensors=\"pt\").to(llama_model.device)\n",
        "output = llama_model.generate(**input_ids, max_new_tokens=16)"
      ],
      "metadata": {
        "id": "unPXebajIRiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama_tokenizer.decode(output[0])"
      ],
      "metadata": {
        "id": "b666SALzaTbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demonstration capturing activations and using SAEs\n",
        "Using Llama 3.1 8B base model"
      ],
      "metadata": {
        "id": "VdFY7p-iqN9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo of how to register hook and run the model with it (and remove hook at the end)\n",
        "\n",
        "model = llama_model\n",
        "tokenizer = llama_tokenizer\n",
        "\n",
        "# Example\n",
        "input_text = \"Consider the equation 3x - 1 = 2. What is x? Answer: x = \"\n",
        "input_ids = llama_tokenizer(input_text, return_tensors=\"pt\").to(llama_model.device)\n",
        "\n",
        "# Dictionary to capture activations\n",
        "activations = {}\n",
        "\n",
        "# Hook function to capture activations\n",
        "def get_activation(name):\n",
        "    def hook(module, input, output):\n",
        "        if isinstance(output, tuple):\n",
        "            act = output[0]\n",
        "        else:\n",
        "            act = output\n",
        "        activations[name] = act.detach().cpu()  # transfer to CPU immediately\n",
        "    return hook\n",
        "\n",
        "# Register hook on the target layer\n",
        "target_layer = model.base_model.layers[15].mlp\n",
        "hook_handle = target_layer.register_forward_hook(get_activation(\"layer15_mlp\"))\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**input_ids, max_new_tokens=48)\n",
        "\n",
        "print(\"Output:\")\n",
        "print(tokenizer.decode(output[0]))\n",
        "\n",
        "hook_handle.remove()"
      ],
      "metadata": {
        "id": "7qcz1exU6W8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve activations from CPU\n",
        "layer_activation = activations.get(\"layer15_mlp\")\n",
        "if layer_activation is None:\n",
        "    raise RuntimeError(\"Failed to capture activation from the target layer.\")"
      ],
      "metadata": {
        "id": "HjCOVDQUZOLf"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_activation.shape"
      ],
      "metadata": {
        "id": "21mtQBzCicrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_activation"
      ],
      "metadata": {
        "id": "KJmGao2Rhreg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(layer_activation.flatten().numpy(), \".\")"
      ],
      "metadata": {
        "id": "3Mkve5_ChzvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sae = load_llama_scope_sae(15, \"m\", 8)"
      ],
      "metadata": {
        "id": "LE1Jfcaol5zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_representation = sae(layer_activation)"
      ],
      "metadata": {
        "id": "p0KbV52yiJr4"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer 15 SAE representation value counts\n",
        "plt.hist(sparse_representation.flatten().data.type(torch.float32), bins=100);"
      ],
      "metadata": {
        "id": "nCl6SlhviZN7",
        "outputId": "d4c3f69d-b215-4108-d6b7-22c900724437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MMLU exploration\n",
        "Let's use the `elementary_mathematics` as default"
      ],
      "metadata": {
        "id": "p97aB48tqmYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mmlu_dataset = load_mmlu_dataset(config_name=\"elementary_mathematics\")\n",
        "print(mmlu_dataset)"
      ],
      "metadata": {
        "id": "w5sceunkrDzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mmlu_data_prompt(dataset, example_idx, dataset_split):\n",
        "    if not (dataset_split in [\"test\", \"validation\", \"dev\"]):\n",
        "        raise ValueError(\"Invalid dataset split. Must be 'test', 'validation', or 'dev'.\")\n",
        "\n",
        "    example = dataset[dataset_split]\n",
        "    question = example[\"question\"][example_idx]\n",
        "    subject = example[\"subject\"][example_idx]\n",
        "    choices = example[\"choices\"][example_idx]\n",
        "    answer = example[\"answer\"][example_idx]\n",
        "\n",
        "    prompt = f\"Subject: {subject}\\n\\n\"\n",
        "    prompt += f\"Question: {question}\\n\\n\"\n",
        "    prompt += \"Choices:\\n\"\n",
        "\n",
        "    letters = \"ABCD\"\n",
        "    for i, choice in enumerate(choices):\n",
        "        prompt += f\"{letters[i]}. {choice}\\n\"\n",
        "\n",
        "    # TODO room for reasoning. i.e. <think> <\\think> block and final answer\n",
        "    # prompt += \"\\nExplanations (finish by writing Answer: <your answer>)\"\n",
        "\n",
        "    # prompt += \"\\nAnswer:\"\n",
        "\n",
        "    prompt += \"\\nLet's solve this problem step by step:\\n\"\n",
        "\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "mHdQbPvMrxGT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_demo_exmaple = mmlu_data_prompt(mmlu_dataset, 0, \"test\")\n",
        "print(prompt_demo_exmaple)"
      ],
      "metadata": {
        "id": "NXO6zLKHwODc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E.g. using the Llama 3.1 8B base model"
      ],
      "metadata": {
        "id": "McWnViiG1NbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = mmlu_data_prompt(mmlu_dataset, 0, \"test\")\n",
        "input_ids = llama_tokenizer(input_text, return_tensors=\"pt\").to(llama_model.device)\n",
        "output = llama_model.generate(**input_ids, max_new_tokens=32)\n",
        "output_text = llama_tokenizer.decode(output[0])\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "G-dmqBwBxkQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try instead the R1 (8B distilled) reasoning model"
      ],
      "metadata": {
        "id": "6WvaVRr-1WUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete Llama model from GPU\n",
        "try:\n",
        "    del model\n",
        "    del tokenizer\n",
        "    del llama_model\n",
        "    del llama_tokenizer\n",
        "    torch.cuda.empty_cache()\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "t546DgVu27hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, model = load_r1_8B_model() # Loads to GPU as default"
      ],
      "metadata": {
        "id": "-8PMl0t71JTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = mmlu_data_prompt(mmlu_dataset, 0, \"test\")\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "output = model.generate(**input_ids, max_new_tokens=128)\n",
        "output_text = tokenizer.decode(output[0])\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "HNlIOnQa3jDm",
        "outputId": "9156775f-2439-449a-b9bd-0b381c758d8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<｜begin▁of▁sentence｜>Subject: elementary_mathematics\n",
            "\n",
            "Question: What is the value of p in 24 = 2p?\n",
            "\n",
            "Choices:\n",
            "A. p = 4\n",
            "B. p = 8\n",
            "C. p = 12\n",
            "D. p = 24\n",
            "\n",
            "Let's solve this problem step by step:\n",
            "1. We have the equation 24 = 2p.\n",
            "2. To find the value of p, we need to solve for p.\n",
            "3. We can do this by dividing both sides of the equation by 2.\n",
            "4. So, p = 24 / 2.\n",
            "5. Calculating that gives p = 12.\n",
            "6. Therefore, the correct answer is C. p = 12.\n",
            "\n",
            "But wait, let me double-check. If p is 12, then 2 times 12 is indeed 24. So, yes, that makes sense.\n",
            "\n",
            "Alternatively, thinking about it in another way: if I have 24 and I want to find out what number multiplied by 2 gives me 24, that number must be 12. So, p is 12.\n",
            "\n",
            "I don't see any mistakes in this reasoning. It seems straightforward. So, the answer should definitely be C.\n",
            "</think>\n",
            "\n",
            "**Question:** What is the value of \\( p \\) in \\( 24 = 2p \\)?\n",
            "\n",
            "**Choices:**\n",
            "A. \\( p = 4 \\)\n",
            "B. \\( p = 8 \\)\n",
            "C. \\( p = 12 \\)\n",
            "D. \\( p = 24 \\)\n",
            "\n",
            "**Solution:**\n",
            "\n",
            "To find the value of \\( p \\) in the equation \\( 24 = 2p \\), follow these steps:\n",
            "\n",
            "1. **Start with the equation:**\n",
            "   \\[\n",
            "   24 = 2p\n",
            "   \\]\n",
            "   \n",
            "2. **To solve for \\( p \\), divide both sides of the equation by 2:**\n",
            "   \\[\n",
            "   \\frac{24}{2} = \\frac{2p}{2}\n",
            "   \\]\n",
            "   \n",
            "3. **Simplify both sides:**\n",
            "   \\[\n",
            "   12 = p\n",
            "   \\]\n",
            "   \n",
            "4. **Therefore, the value of \\( p \\) is 12.**\n",
            "\n",
            "**Verification:**\n",
            "- If \\( p = 12 \\), then \\( 2p = 2 \\times 12 = 24 \\), which matches the original equation.\n",
            "\n",
            "**Answer:** **C. \\( p = 12 \\)**<｜end▁of▁sentence｜>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lx7o3y7G7RDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleen-up"
      ],
      "metadata": {
        "id": "axuiwVFKrERL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Free GPU memory\n",
        "# del model  # remove the model instance\n",
        "# torch.cuda.empty_cache()  # clear cached memory\n",
        "# # gc.collect() # Necessary?"
      ],
      "metadata": {
        "id": "EN7utrDyZgRR"
      },
      "execution_count": 36,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}